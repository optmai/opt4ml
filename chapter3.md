---
layout: page
title: Ch3
nav_order: 4
---

## Basics â€” Stochastic Optimization

> *Stochastic optimization is classical wisdom in motion!*

In this chapter, we introduce standard stochastic optimization problems and present key stochastic optimization algorithms along with their complexity analysis. While many important stochastic algorithms have been proposed for solving stochastic optimization and empirical risk minimization problems, we focus on seven foundational methods that gained prominence before the deep learning era. These algorithms have had a profound impact on machine learning and provide essential insights for understanding more advanced methods presented in later chapters. The selected algorithms include stochastic gradient descent (SGD), stochastic proximal gradient descent, stochastic mirror descent, adaptive gradient methods, stochastic coordinate descent, stochastic gradient descent-ascent, and stochastic optimistic mirror prox. We concentrate on the complexity analysis in the convex setting.

---

## Contents

- [3.1 Stochastic Gradient Descent](Ch3-1.md)
- [3.2 Stochastic Proximal Gradient Descent](Ch3-2.md)
- [3.3 Stochastic Coordinate Descent](Ch3-3.md)
- [3.4 Stochastic Mirror Descent](Ch3-4.md)
- [3.5 Adaptive Gradient Method (AdaGrad)](Ch3-5.md)
- [3.6 Stochastic Gradient Descent Ascent](Ch3-6.md)
- [3.7 Stochastic Optimistic Mirror Prox](Ch3-7.md)
- [3.8 Notes and Discussion](Ch3-8.md)

