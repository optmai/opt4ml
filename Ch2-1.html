<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Section 2.1: Empirical Risk Minimization</title>
  <style>
    html {
      font-family: DejaVu Sans;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <style>
    body {
      max-width: 750px;
      margin: 2rem auto;
      padding: 2rem;
      font-family: Merriweather, Palatino, "Palatino Linotype", "Palatino LT STD", "Book Antiqua", Georgia, serif;
      font-size: 16.8px;    
      line-height: 28.8px;
      background-color: #ffffff;
      color: #000000;
    }

    .back-link {
      font-size: 1rem;
      margin-bottom: 1rem;
      display: inline-block;
      text-decoration: none;
      color: #0366d6;
    }

    .back-link:hover {
      text-decoration: underline;
    }

    .share-buttons {
      margin: 1rem 0;
      display: flex;
      gap: 10px;
    }

    .share-buttons button {
      background: none;
      border: none;
      cursor: pointer;
      padding: 0;
      width: 32px;
      height: 32px;
    }

    .share-buttons svg {
      width: 100%;
      height: 100%;
      fill: #555;
    }

    .share-buttons button:hover svg {
      fill: #000;
    }

  span.math.display {
    display: block;
    overflow-x: auto;
    white-space: nowrap;
    padding: 0.5em 0;
    max-width: 100%;
    box-sizing: border-box;
  }

  /* Wrap display math equations to prevent overflow */
  mjx-container[jax="CHTML"][display="true"] {
    display: block;
    overflow-x: auto;
    overflow-y: hidden;
    text-align: left;
    padding: 0.5em 0;
    max-width: 100%;
    box-sizing: border-box;
  }


  /* Ensure inner equations don't break layout on small screens */
  mjx-container > svg {
    max-width: 100% !important;
    height: auto !important;
  }

  @media screen and (orientation: landscape) and (max-width: 900px) {
    mjx-container[jax="CHTML"] {
      font-size: 24.5px !important; /* or try 18.5px */
    }
  }

  </style>

  <a href="javascript:history.back()" class="back-link">‚Üê Go Back</a>

  <div class="share-buttons">
    <!-- X icon -->
    <button onclick="shareOnX()" title="Share on X">
      <svg viewBox="0 0 24 24"><path d="M14.23 10.45 22.12 2h-2.09l-6.77 7.16L7.71 2H2l8.3 11.8L2 22h2.09l7.18-7.61 5.94 7.61H22l-7.77-11.55zm-2.55 2.71-.83-1.14L4.34 3.5h2.72l5.1 6.99.84 1.14 6.41 8.78h-2.71l-5.02-6.75z"/></svg>
    </button>

    <!-- LinkedIn icon -->
    <button onclick="shareOnLinkedIn()" title="Share on LinkedIn">
      <svg viewBox="0 0 24 24"><path d="M20.45 20.45h-3.63V15c0-1.3-.03-2.97-1.81-2.97-1.82 0-2.1 1.42-2.1 2.87v5.55H9.29V9h3.49v1.56h.05c.48-.9 1.65-1.84 3.39-1.84 3.63 0 4.3 2.39 4.3 5.5v6.23zM5.34 7.43a2.1 2.1 0 1 1 0-4.2 2.1 2.1 0 0 1 0 4.2zM7.15 20.45H3.54V9h3.61v11.45zM22.22 0H1.78C.8 0 0 .78 0 1.74v20.52C0 23.2.8 24 1.78 24h20.44c.98 0 1.78-.8 1.78-1.74V1.74C24 .78 23.2 0 22.22 0z"/></svg>
    </button>
  </div>

  <script>
    function shareOnX() {
      const url = encodeURIComponent(window.location.href);
      const text = encodeURIComponent(document.title || 'Check this out');
      window.open(`https://twitter.com/intent/tweet?url=${url}&text=${text}`, '_blank');
    }

    function shareOnLinkedIn() {
      const url = encodeURIComponent(window.location.href);
      window.open(`https://www.linkedin.com/sharing/share-offsite/?url=${url}`, '_blank');
    }
  </script>

  <!-- MathJax -->
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        tags: 'ams'
      },
     chtml: {
      scale: 1
     }
    };
  </script>
  <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
</head>
<body>
<article class="markdown-body">
<header id="title-block-header">
<h1 class="title">Section 2.1: Empirical Risk Minimization</h1>
</header>
<h1 id="sec:1">Empirical Risk Minimization</h1>
<div
style="background-color:#f2f7ff; border:1px solid #1f4aa8; border-radius:0px; padding:0.6em 0.8em; overflow-x:auto;">
<div style="font-weight:700; margin-bottom:0.4em;">
What is Machine Learning (ML)?
</div>
In 1959, Arthur Samuel, a pioneer in the field of ML, defined Machine
Learning as the ‚Äúfield of study that gives computers the ability to
learn without being explicitly programmed‚Äù.
</div>
<p>Nowadays, machine learning has become the foundation of AI. The
essence of machine learning is to learn a model by optimizing an
objective function on training data, with the goal of achieving strong
generalization to unseen data. This relationship is captured by the
formula: <span class="math display">\[\begin{align*}
\text{\bf Machine Learning}=\text{\bf Objective}\quad+\quad\text{\bf
Algorithm}\quad+\quad\text{\bf Generalization}.
\end{align*}\]</span> Optimization plays a fundamental role in machine
learning, as it underpins (1) the formulation of objective functions,
(2) the development of optimization algorithms, and (3) the analysis of
generalization error of learned models. Below, we will use the
traditional label prediction problem to illustrate the three
components.</p>
<h2 id="discriminative-label-prediction">Discriminative Label
Prediction</h2>
<p>In supervised learning, the primary objective is often to learn a
predictive model from a given set of supervised training data. Let us
consider a classical label prediction problem. Denote by <span
class="math inline">\((\mathbf{x},y)\)</span> a data-label pair, where
<span
class="math inline">\(\mathbf{x}\in\mathcal{X}\subset\mathbb{R}^{d_0}\)</span>
denotes the input feature vector, and <span
class="math inline">\(y\in\mathcal{Y}=\{1,\ldots,K\}\)</span> is the
corresponding label. The goal is to learn a predictive model
parameterized by <span
class="math inline">\(\mathbf{w}\in\mathcal{W}\subseteq\mathbb{R}^d\)</span>
(e.g., a deep neural network), which induces a scoring function <span
class="math inline">\(h(\mathbf{w};\cdot):\mathcal{X}\to\mathbb{R}^K\)</span>.
Conceptually, the model can be expressed as <span
class="math inline">\(h(\mathbf{w};\mathbf{x})=Wh_0(\mathbf{w};\mathbf{x})\)</span>,
where <span
class="math inline">\(h_0(\mathbf{w};\cdot):\mathcal{X}\to\mathbb{R}^{d_1}\)</span>
is the feature extraction component, and <span
class="math inline">\(W\in\mathbb{R}^{K\times d_1}\)</span> is the
classification head corresponding to the <span
class="math inline">\(K\)</span> classes.</p>
<p>A classical framework for learning such a model is the well-known
empirical risk minimization (ERM), which minimizes the empirical risk
over the training dataset. To this end, a pointwise loss function <span
class="math inline">\(\ell(h(\mathbf{w};\mathbf{x}),y)\)</span> is
defined to measure the discrepancy between the model‚Äôs prediction <span
class="math inline">\(h(\mathbf{w};\mathbf{x})\)</span> and the true
label <span class="math inline">\(y\)</span>. Given a training dataset
<span
class="math inline">\(\mathcal{S}=\{(\mathbf{x}_1,y_1),\ldots,(\mathbf{x}_n,y_n)\}\)</span>,
the ERM problem is formulated as: <span
class="math display">\[\begin{align}\label{eqn:erm}
\min_{\mathbf{w}\in\mathcal{W}}\mathcal{R}_{\mathcal{S}}(\mathbf{w}):=\frac{1}{n}\sum_{i=1}^n\ell(h(\mathbf{w};\mathbf{x}_i),y_i).
\end{align}\]</span></p>
<h2 id="discriminative-loss-functions">Discriminative Loss
Functions</h2>
<p>A major element of ERM is the design of the loss function. A common
strategy of designing a loss function for label prediction is through a
discriminative approach. Below, we introduce several discriminative loss
functions.</p>
<h3 id="logistic-loss">Logistic Loss</h3>
<p>A parameterized probabilistic model is defined to represent the
probability of a class label for a given data point as <span
class="math display">\[\begin{align}\label{eqn:dpm-class}
\Pr(y|\mathbf{x};\mathbf{w})=\frac{\exp([h(\mathbf{w};\mathbf{x})]_y)}{\sum_{l=1}^K\exp([h(\mathbf{w};\mathbf{x})]_l)},
\end{align}\]</span> where <span
class="math inline">\([\cdot]_k\)</span> denotes the <span
class="math inline">\(k\)</span>-th element of a vector. The associated
loss is derived from the negative log-likelihood, resulting in the
multi-class logistic loss, also known as the cross-entropy (CE) loss:
<span class="math display">\[\begin{align}\label{eqn:lsr}
\ell(h(\mathbf{w};\mathbf{x}),y)=-\log\frac{\exp([h(\mathbf{w};\mathbf{x})]_y)}{\sum_{l=1}^K\exp([h(\mathbf{w};\mathbf{x})]_l)}.
\end{align}\]</span> The resulting method by ERM is commonly referred to
as multi-class logistic regression. For binary classification, this loss
becomes the binary logistic loss <span
class="math inline">\(\ell(h(\mathbf{w};\mathbf{x}),y)=\log(1+\exp(-yh(\mathbf{w};\mathbf{x})))\)</span>,
where <span
class="math inline">\(h(\mathbf{w};\cdot)\in\mathbb{R}\)</span> and
<span class="math inline">\(y\in\{1,-1\}\)</span>.</p>
<h3 id="max-margin-loss">Max-Margin Loss</h3>
<p>The max-margin loss, introduced by Crammer and Singer and commonly
referred to as the Crammer-Singer (CS) loss, is defined as: <span
class="math display">\[\begin{align}
\ell(h(\mathbf{w};\mathbf{x}),y)=\max\left(0,\max_{k\ne
y}\left(c_{k,y}+[h(\mathbf{w};\mathbf{x})]_k-[h(\mathbf{w};\mathbf{x})]_y\right)\right),
\end{align}\]</span> where <span
class="math inline">\(c_{k,y}&gt;0\)</span> is a margin parameter. This
loss seeks to ensure that the prediction score for the ground-truth
label, <span
class="math inline">\([h(\mathbf{w};\mathbf{x})]_y\)</span>, exceeds the
scores of other class labels, <span
class="math inline">\([h(\mathbf{w};\mathbf{x})]_k\)</span> for <span
class="math inline">\(k\ne y\)</span>, by at least the margin <span
class="math inline">\(c_{k,y}\)</span>. This method is also known as the
multi-class support vector machine. For binary classification, it
reduces to the standard hinge loss <span
class="math inline">\(\ell(h(\mathbf{w};\mathbf{x}),\mathbf{y})=\max(0,1-yh(\mathbf{w};\mathbf{x}))\)</span>
for <span
class="math inline">\(h(\mathbf{w};\cdot)\in\mathbb{R}\)</span> and
<span class="math inline">\(y\in\{1,-1\}\)</span> with a margin <span
class="math inline">\(1\)</span>.</p>
<h3 id="label-distributionally-robust-ldr-loss">Label Distributionally
Robust (LDR) Loss</h3>
<p>Both the CS loss and the CE loss have their strengths and
limitations. For example, the CS loss with the margin parameters is more
flexible in controlling the discrimination between classes, while it is
not consistent and non-smooth in terms of the prediction scores. The CE
loss is smooth and consistent but lacks robustness to noise in class
labels.</p>
<div
style="background-color:#f2f7ff; border:1px solid #1f4aa8; border-radius:0px; padding:0.6em 0.8em; overflow-x:auto;">
<div style="font-weight:700; margin-bottom:0.4em;">
Consistency of a surrogate loss function
</div>
The consistency measures whether minimizing a surrogate loss with an
infinite number of data also minimizes the Bayes error. More formally, a
surrogate loss <span
class="math inline">\(\ell(h(\mathbf{x}),y)\)</span> is said to be
consistent if for any sequence of measurable functions <span
class="math inline">\(h^{(n)}\)</span> it holds <span
class="math display">\[
\mathcal{R}(h^{(n)})\rightarrow\inf_{h\in\mathcal{H}}\mathcal{R}(h)\Rightarrow\mathcal{R}_{0-1}(h^{(n)})\rightarrow\inf_{h\in\mathcal{H}}\mathcal{R}_{0-1}(h),
\]</span> where <span
class="math inline">\(\mathcal{R}(h)=\mathbb{E}_{\mathbf{x},y}[\ell(h(\mathbf{x}),y)]\)</span>
is the expected risk, <span
class="math inline">\(\mathcal{R}_{0-1}(h)=\mathbb{E}_{\mathbf{x},y}\!\left[\mathbb{I}\!\left(y\ne
h(\mathbf{x})\right)\right]\)</span> is the Bayes error, and <span
class="math inline">\(\mathcal{H}\)</span> is the set of any measurable
functions.
</div>
<p>In fact, the strengths and limitations of both the CE and CS losses
can be better understood within a broader family known as the
label-distributionally robust (LDR) loss: <a id="eqn-ldr"></a> <span
class="math display">\[\begin{align}\label{eqn:ldr}
\ell_\tau(h(\mathbf{w};\mathbf{x}),y)=\max_{\mathbf{p}\in\Delta_K}\sum_{k=1}^K
p_k\big([h(\mathbf{w};\mathbf{x})]_k-[h(\mathbf{w};\mathbf{x})]_y+c_{k,y}\big)-\tau\sum_{k=1}^K
p_k\log(p_k K),
\end{align}\]</span> where <span
class="math inline">\(\tau&gt;0\)</span> is a hyperparameter, <span
class="math inline">\(c_{y,y}=0\)</span>, <span
class="math inline">\(\mathbf{p}\in\mathbb{R}^K\)</span> is referred to
as the label distributional weight vector, and <span
class="math inline">\(\Delta_K=\{\mathbf{p}\in\mathbb{R}^K:p_k\ge0,\sum_{k=1}^K
p_k=1\}\)</span> is a simplex.</p>
<p>It is clear that the LDR loss is defined by solving an optimization
problem. Indeed, the above optimization problem follows the
distributionally robust optimization (DRO) principle, which is widely
used at the level of data as discussed in <a href="Ch2-2.html">Section
2.2</a>. By treating `label‚Äô as a kind of data, we can unify the LDR
loss with other losses discussed later in <a href="Ch2-4.html">Section
2.4</a>.</p>
<p>A closed-form solution for <span
class="math inline">\(\mathbf{p}\)</span> can be derived using the KKT
conditions (cf.¬†<a href="Ch1-4.html#example16-kkt">Example 1.16</a>,
making the LDR loss equivalent to: <span
class="math display">\[\begin{align}\label{eqn:ldr-kl}
\ell_\tau(h(\mathbf{w};\mathbf{x}),y)=\tau\log\bigg(\frac{1}{K}\sum_{k=1}^K\exp\left(\frac{[h(\mathbf{w};\mathbf{x})]_k+c_{k,y}-[h(\mathbf{w};\mathbf{x})]_y}{\tau}\right)\bigg).
\end{align}\]</span></p>
<p>From the perspective of DRO, we can define a more general family of
LDR losses using different regularization functions on <span
class="math inline">\(\mathbf{p}\)</span> and constrained domains <span
class="math inline">\(\Omega\)</span>: <span
class="math display">\[\begin{align}\label{eqn:ldr-g}
\bar\ell_\tau(h(\mathbf{w};\mathbf{x}),y)=\max_{\mathbf{p}\in\Omega}\sum_{k=1}^K
p_k\big([h(\mathbf{w};\mathbf{x})]_k-[h(\mathbf{w};\mathbf{x})]_y+c_{k,y}\big)-\tau
R(\mathbf{p}).
\end{align}\]</span> where <span
class="math inline">\(\Omega\subseteq\Delta_K\)</span> and <span
class="math inline">\(R(\mathbf{p})\)</span> is a strongly convex
regularizer.</p>
<div
style="border: 2px solid #aaa; padding: 0.6em 0.8em; overflow-x: auto; border-radius: 6px; background-color: #f0f0f0;">
<p><strong>üí° Why it matters</strong><br />
- The LDR loss (<span class="math inline">\(\ref{eqn:ldr-kl}\)</span>)
unifies both the CS and CE losses as special cases. Specifically, the CE
loss corresponds to the LDR loss when <span
class="math inline">\(\tau=1\)</span> and <span
class="math inline">\(c_{k,y}=0\)</span> for all <span
class="math inline">\(k\)</span>, while the CS loss corresponds to the
case <span class="math inline">\(\tau=0\)</span>.</p>
<p>Moreover, the LDR loss encompasses the Label-Distribution-Aware
Margin (LDAM) loss <a href="Ch2-5.html#ref2">(Cao et al.¬†2019)</a> when
<span class="math inline">\(\tau=1\)</span> and <span
class="math inline">\(c_{k,y}=c_y\propto1/n_y^{1/4}\)</span> for <span
class="math inline">\(k\ne y\)</span>, where <span
class="math inline">\(n_y\)</span> denotes the number of samples in
class <span class="math inline">\(y\)</span>: <span
class="math display">\[ \begin{align*}
  \ell_{\text{LDAM}}(h(\mathbf{w};\mathbf{x}),y)
  &amp;=-\log\left(\frac{\exp\big([h(\mathbf{w};\mathbf{x})]_y-\frac{C}{n_y^{1/4}}\big)}{\exp\big([h(\mathbf{w};\mathbf{x})]_y-\frac{C}{n_y^{1/4}}\big)+\sum_{l\ne
y}\exp([h(\mathbf{w};\mathbf{x})]_l)}\right),
  \end{align*}\]</span> where <span class="math inline">\(C\)</span> is
a constant. For imbalanced datasets, this assigns larger margins <span
class="math inline">\(c_y\)</span> to minority classes, making it more
suitable for handling class imbalance.</p>
<ul>
<li><p>The LDR loss provides insights into the strengths and limitations
of CE and CS losses. The regularizer <span
class="math inline">\(R(\mathbf{p})=\sum_{k=1}^K p_k\log(p_k K)\)</span>
is strongly convex in <span class="math inline">\(\mathbf{p}\)</span>,
which implies smoothness of the loss in terms of prediction scores due
to the duality between smoothness and strong convexity (See <a
href="Ch1-5.html#lem:sm-sc-duality">Lemma 1.9</a>). This strong
convexity also contributes to the statistical consistency of the loss <a
href="Ch2-5.html#ref3">(Zhu et al.¬†2023)</a> . In contrast, the CS loss
with <span class="math inline">\(\tau=0\)</span> lacks this property,
and hence suffer from non-smoothness and inconsistency.</p></li>
<li><p>The LDR loss framework enables the design of new losses that are
robust to label noise. For instance, when <span
class="math inline">\(\tau\to\infty\)</span>, the LDR loss reduces to:
<span class="math display">\[\begin{align*}
\ell_\infty(\mathbf{w};\mathbf{x},y)=\frac{1}{K}\sum_{k=1}^K\left([h(\mathbf{w};\mathbf{x})]_k-[h(\mathbf{w};\mathbf{x})]_y+c_{k,y}\right).
\end{align*}\]</span> A remarkable property of this loss is its
symmetry: <span
class="math inline">\(\sum_{y=1}^K\ell_\infty(\mathbf{w};\mathbf{x},y)\)</span>
is constant. This symmetry serves as a sufficient condition for
robustness to uniform label noise <a href="Ch2-5.html#ref6">(Ghosh et
al.¬†2017)</a> However, by treating all negative labels equally, it may
limit the model‚Äôs ability to focus on hard negative labels and
potentially slow down the learning process. In practice, it is better to
tune <span class="math inline">\(\tau\)</span> if there is label
noise.</p></li>
</ul>
</div>
<p>In conclusion, the LDR loss offers flexibility in achieving three
desirable properties: max-margin, consistency, and symmetry. In
practice, when tuning <span
class="math inline">\(\tau\in(0,\infty)\)</span>, it may be beneficial
to normalize the prediction scores <span
class="math inline">\(h(\mathbf{w};\mathbf{x})\)</span>.</p>
<div
style="background-color:#f2f7ff; border:1px solid #1f4aa8; border-radius:0px; padding:0.6em 0.8em; overflow-x:auto;">
<div style="font-weight:700; margin-bottom:0.4em;">
Critical:
</div>
It is worth noting that all the discussed losses are discriminative in
nature, aiming to increase the score <span
class="math inline">\([h(\mathbf{w};\mathbf{x})]_y\)</span> of the true
label while decreasing the scores <span
class="math inline">\([h(\mathbf{w};\mathbf{x})]_k\)</span> of the
negative labels (<span class="math inline">\(k\ne y\)</span>).
</div>
<figure id="fig:enter-label">
<img src="assets/LDR-loss.png" alt="The LDR loss and its special cases by varying tau."  style="width: 100%; max-width: 900px;">
<figcaption style="text-align: center; font-style: italic; margin-top: 0.5em;">
The LDR loss and its special cases by varying <span
class="math inline">\(\tau\)</span>.
</figcaption>
</figure>
<h2 id="need-of-optimization-algorithms">Need of Optimization
Algorithms</h2>
<p>To address the ERM problem in the context of large-scale data (i.e.,
a substantial number of training examples), first-order stochastic
algorithms are commonly employed. These include stochastic gradient
descent (SGD), stochastic momentum methods, and adaptive gradient
methods. For instance, the update rule of classical SGD for solving
(<span class="math inline">\(\ref{eqn:erm}\)</span>) with <span
class="math inline">\(\mathcal{W}=\mathbb{R}^d\)</span> is given by:
<span class="math display">\[\begin{align}
\mathbf{w}_{t+1}=\mathbf{w}_t-\eta_t\frac{1}{|\mathcal{B}_t|}\sum_{(\mathbf{x}_i,y_i)\in\mathcal{B}_t}\nabla\ell(h(\mathbf{w}_t;\mathbf{x}_i),y_i),\quad
t=1,\ldots,T,
\end{align}\]</span> where <span
class="math inline">\(\eta_t\ge0\)</span> is the learning rate (or step
size), and <span class="math inline">\(\mathcal{B}_t\)</span> denotes a
random mini-batch data sampled from the full dataset. The concern of
designing an optimization algorithm is how fast the algorithm can
converge to a (near) optimal solution. We will discuss the design and
analysis of classical stochastic optimization algorithms in <a
href="chapter3">Chapter 3</a>.</p>
<div
style="background-color:#f2f7ff; border:1px solid #1f4aa8; border-radius:0px; padding:0.6em 0.8em; overflow-x:auto;">
<div style="font-weight:700; margin-bottom:0.4em;">
Critical:
</div>
A critical assumption in conventional stochastic optimization algorithms
such as SGD is that the gradient <span
class="math inline">\(\nabla\ell(h(\mathbf{w};\mathbf{x}_i),y_i)\)</span>
of each individual loss can be easily computed. This assumption will
fail for the logistic loss when the number of classes <span
class="math inline">\(K\)</span> is gigantic, e.g.¬†millions or even
billions. This challenge will be addressed in this book.
</div>
<h2 id="generalization-analysis">Generalization Analysis</h2>
<p>To study the generalization of a model learned by solving ERM, we
usually consider the expected risk defined as <span
class="math display">\[\begin{align}\label{eqn:pr}
\mathcal{R}(\mathbf{w})=\mathbb{E}_{\mathbf{x},y\sim\mathbb{P}}[\ell(h(\mathbf{w};\mathbf{x}),y)].
\end{align}\]</span> Let <span
class="math inline">\(\mathbf{w}=\mathcal{A}(\mathcal{S};\zeta)\)</span>
denote a learned model by a randomized algorithm <span
class="math inline">\(\mathcal{A}\)</span> for solving ERM that depend
on random variables <span class="math inline">\(\zeta\)</span>. A
standard measure of generalization is given by the excess risk defined
as <span
class="math inline">\(\mathcal{R}(\mathbf{w})-\mathcal{R}(\mathbf{w}_*)\)</span>,
where <span
class="math inline">\(\mathbf{w}_*\in\arg\min_{\mathbf{u}\in\mathcal{W}}\mathcal{R}(\mathbf{u})\)</span>.
The following lemma decomposes the excess risk into the optimization
error and the generalization error.</p>
<div id="lem:decomp"
style="border: 5px solid #ccc; padding: 0.2em; overflow-x: auto; border-radius: 6px; background-color: #eef4fc;">
<p><strong><strong>Lemma 2.1</strong> </strong><br />
For a learned model <span
class="math inline">\(\mathbf{w}=\mathcal{A}(\mathcal{S};\zeta)\in\mathcal{W}\)</span>,
we have <span class="math display">\[\begin{align*}
\mathcal{R}(\mathbf{w})-\mathcal{R}(\mathbf{w}_*)
\le2\underbrace{\sup_{\mathbf{w}\in\mathcal{W}}|\mathcal{R}(\mathbf{w})-\mathcal{R}_{\mathcal{S}}(\mathbf{w})|}\limits_{\text{generalization
error}}
+\underbrace{\mathcal{R}_{\mathcal{S}}(\mathbf{w})-\min_{\mathbf{u}\in\mathcal{W}}\mathcal{R}_{\mathcal{S}}(\mathbf{u})}\limits_{\text{optimization
error}},
\end{align*}\]</span> and <span class="math display">\[\begin{align*}
\mathbb{E}_{\mathcal{S},\zeta}[\mathcal{R}(\mathbf{w})-\mathcal{R}(\mathbf{w}_*)]
=\mathbb{E}_{\mathcal{S},\zeta}[\mathcal{R}(\mathbf{w})-\mathcal{R}_{\mathcal{S}}(\mathbf{w})]
+\mathbb{E}_{\mathcal{S},\zeta}\left[\mathcal{R}_{\mathcal{S}}(\mathbf{w})-\min_{\mathbf{u}\in\mathcal{W}}\mathcal{R}_{\mathcal{S}}(\mathbf{u})\right].
\end{align*}\]</span></p>
</div>
<strong>Proof.</strong> <span class="math display">\[\begin{align*}
\mathcal{R}(\mathbf{w})-\mathcal{R}(\mathbf{w}_*)
&amp;=\mathcal{R}(\mathbf{w})-\mathcal{R}_{\mathcal{S}}(\mathbf{w})
+\mathcal{R}_{\mathcal{S}}(\mathbf{w})-\min_{\mathbf{u}\in\mathcal{W}}\mathcal{R}_{\mathcal{S}}(\mathbf{u})
+\min_{\mathbf{u}\in\mathcal{W}}\mathcal{R}_{\mathcal{S}}(\mathbf{u})-\mathcal{R}(\mathbf{w}_*)\\
&amp;\le\mathcal{R}(\mathbf{w})-\mathcal{R}_{\mathcal{S}}(\mathbf{w})
+\mathcal{R}_{\mathcal{S}}(\mathbf{w})-\min_{\mathbf{u}\in\mathcal{W}}\mathcal{R}_{\mathcal{S}}(\mathbf{u})
+\mathcal{R}_{\mathcal{S}}(\mathbf{w}_*)-\mathcal{R}(\mathbf{w}_*).
\end{align*}\]</span> This proves the first inequality. By taking
expectation over <span class="math inline">\(\mathcal{S},\zeta\)</span>
and noting that <span
class="math inline">\(\mathbb{E}_{\mathcal{S}}[\mathcal{R}_{\mathcal{S}}(\mathbf{w}_*)-\mathcal{R}(\mathbf{w}_*)]=0\)</span>,
we finish the second inequality.<br />

<p style="text-align: right;">
‚ñ†
</p>
<div
style="border: 2px solid #aaa; padding: 0.6em 0.8em; overflow-x: auto; border-radius: 6px; background-color: #f0f0f0;">
<p><strong>üí° Why it matters</strong><br />
The excess risk can be decomposed into two components: the optimization
error, given by <span
class="math inline">\(\mathcal{R}_{\mathcal{S}}(\mathbf{w})-\min_{\mathbf{u}\in\mathcal{W}}\mathcal{R}_{\mathcal{S}}(\mathbf{u})\)</span>,
and the generalization error which captures the difference between the
expected risk and the empirical risk. Bounding the (expected)
optimization error is a central focus of this book, approached through
the analysis of stochastic optimization algorithms. The generalization
error <span
class="math inline">\(\sup_{\mathbf{w}\in\mathcal{W}}|\mathcal{R}(\mathbf{w})-\mathcal{R}_{\mathcal{S}}(\mathbf{w})|\)</span>
decreases as the training data size <span
class="math inline">\(|\mathcal{S}|\)</span> increases. A brief
discussion of the literature on generalization error analysis will be
provided at the end of this chapter.</p>
</div>
<p style="text-align:left; margin-top:1.5em;">
<a href="javascript:history.back()">‚Üê Go Back</a>
</p>
</article>
</body>
</html>
