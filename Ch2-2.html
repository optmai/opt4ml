<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Section 2.2 Robust Optimization</title>
  <style>
    html {
      font-family: DejaVu Sans;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <style>
    body {
      max-width: 750px;
      margin: 2rem auto;
      padding: 2rem;
      font-family: Merriweather, Palatino, "Palatino Linotype", "Palatino LT STD", "Book Antiqua", Georgia, serif;
      font-size: 16.8px;    
      line-height: 28.8px;
      background-color: #ffffff;
      color: #000000;
    }

    .back-link {
      font-size: 1rem;
      margin-bottom: 1rem;
      display: inline-block;
      text-decoration: none;
      color: #0366d6;
    }

    .back-link:hover {
      text-decoration: underline;
    }

    .share-buttons {
      margin: 1rem 0;
      display: flex;
      gap: 10px;
    }

    .share-buttons button {
      background: none;
      border: none;
      cursor: pointer;
      padding: 0;
      width: 32px;
      height: 32px;
    }

    .share-buttons svg {
      width: 100%;
      height: 100%;
      fill: #555;
    }

    .share-buttons button:hover svg {
      fill: #000;
    }

  span.math.display {
    display: block;
    overflow-x: auto;
    white-space: nowrap;
    padding: 0.5em 0;
    max-width: 100%;
    box-sizing: border-box;
  }

  /* Wrap display math equations to prevent overflow */
  mjx-container[jax="CHTML"][display="true"] {
    display: block;
    overflow-x: auto;
    overflow-y: hidden;
    text-align: left;
    padding: 0.5em 0;
    max-width: 100%;
    box-sizing: border-box;
  }


  /* Ensure inner equations don't break layout on small screens */
  mjx-container > svg {
    max-width: 100% !important;
    height: auto !important;
  }

  @media screen and (orientation: landscape) and (max-width: 900px) {
    mjx-container[jax="CHTML"] {
      font-size: 24.5px !important; /* or try 18.5px */
    }
  }

  </style>

  <a href="javascript:history.back()" class="back-link">‚Üê Go Back</a>

  <div class="share-buttons">
    <!-- X icon -->
    <button onclick="shareOnX()" title="Share on X">
      <svg viewBox="0 0 24 24"><path d="M14.23 10.45 22.12 2h-2.09l-6.77 7.16L7.71 2H2l8.3 11.8L2 22h2.09l7.18-7.61 5.94 7.61H22l-7.77-11.55zm-2.55 2.71-.83-1.14L4.34 3.5h2.72l5.1 6.99.84 1.14 6.41 8.78h-2.71l-5.02-6.75z"/></svg>
    </button>

    <!-- LinkedIn icon -->
    <button onclick="shareOnLinkedIn()" title="Share on LinkedIn">
      <svg viewBox="0 0 24 24"><path d="M20.45 20.45h-3.63V15c0-1.3-.03-2.97-1.81-2.97-1.82 0-2.1 1.42-2.1 2.87v5.55H9.29V9h3.49v1.56h.05c.48-.9 1.65-1.84 3.39-1.84 3.63 0 4.3 2.39 4.3 5.5v6.23zM5.34 7.43a2.1 2.1 0 1 1 0-4.2 2.1 2.1 0 0 1 0 4.2zM7.15 20.45H3.54V9h3.61v11.45zM22.22 0H1.78C.8 0 0 .78 0 1.74v20.52C0 23.2.8 24 1.78 24h20.44c.98 0 1.78-.8 1.78-1.74V1.74C24 .78 23.2 0 22.22 0z"/></svg>
    </button>
  </div>

  <script>
    function shareOnX() {
      const url = encodeURIComponent(window.location.href);
      const text = encodeURIComponent(document.title || 'Check this out');
      window.open(`https://twitter.com/intent/tweet?url=${url}&text=${text}`, '_blank');
    }

    function shareOnLinkedIn() {
      const url = encodeURIComponent(window.location.href);
      window.open(`https://www.linkedin.com/sharing/share-offsite/?url=${url}`, '_blank');
    }
  </script>

  <!-- MathJax -->
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        tags: 'ams'
      },
     chtml: {
      scale: 1
     }
    };
  </script>
  <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
</head>
<body>
<article class="markdown-body">
<header id="title-block-header">
<h1 class="title">Section 2.2 Robust Optimization</h1>
</header>
<p>In this section, we introduce advanced machine learning methods based
on the principle of robust optimization. Robust optimization is a
framework designed to address uncertainty in data. It ensures that the
solutions perform well even under worst-case scenarios of data within a
specified set of uncertainties.</p>
<h2 id="distributionally-robust-optimization">Distributionally Robust
Optimization</h2>
<p>Minimizing the average empirical risk often fails to yield a robust
model in practice. For instance, the resulting model may perform poorly
on minority data (e.g., patients with rare diseases) because the
optimization predominantly focuses on majority class data.</p>
<div
style="background-color:#f2f7ff; border:1px solid #1f4aa8; border-radius:0px; padding:0.6em 0.8em; overflow-x:auto;">
<div style="font-weight:700; margin-bottom:0.4em;">
Critical:
</div>
Empirical data may not fully represent the underlying data distribution,
leading to generalization issues.
</div>
<p>To address these challenges, distributionally robust optimization
(DRO) has been extensively studied in machine learning as a means to
improve robustness and generalization.</p>
<p>The core idea of DRO is to minimize a robust objective defined over
the worst-case distribution of data, perturbed from the empirical
distribution. Let us define a set of distributional weights, <span
class="math inline">\(\mathbf{p}=(p_1,\ldots,p_n)\in\Delta_n\)</span>,
where <span
class="math inline">\(\Delta_n=\{\mathbf{p}\in\mathbb{R}^n:\sum_{i=1}^n
p_i=1,p_i\ge0\}\)</span>, with each element <span
class="math inline">\(p_i\)</span> associated with a training sample
<span class="math inline">\(\mathbf{x}_i\)</span>.</p>
<div
style=" background-color: #f5f7fb; padding: 0.8em 1em; border-radius: 6px; margin: 1em 0;">
<p><strong>Definition 2.1 (<span
class="math inline">\(\phi\)</span>-divergence).</strong><br />
Let <span
class="math inline">\(\phi(t):\mathbb{R}^+\to\mathbb{R}\)</span> be a
proper closed convex function and has a minimum value zero that is
attained at <span class="math inline">\(t=1\)</span>. The <span
class="math inline">\(\phi\)</span>-divergence is defined as: <span
class="math display">\[\begin{align}
D_{\phi}(\mathbf{p}\|\mathbf{q})=\sum_{i=1}^n q_i\phi(p_i/q_i).
\end{align}\]</span></p>
</div>
<p><span class="math inline">\(\phi\)</span>-divergence measures the
discrepancy between two distributions <span
class="math inline">\(\mathbf{p}\)</span> and <span
class="math inline">\(\mathbf{q}\)</span> using the function <span
class="math inline">\(\phi\)</span>. We present two common formulations
of DRO based on the <span
class="math inline">\(\phi\)</span>-divergence: regularized DRO and
constrained DRO. They differ in how to define the uncertainty set of
<span class="math inline">\(\mathbf{p}\)</span>.</p>
<p>Below, we use the generic notation <span
class="math inline">\(\ell(\mathbf{w};\mathbf{z})\)</span> to denote the
loss of a model <span class="math inline">\(\mathbf{w}\)</span> on a
random data point <span class="math inline">\(\mathbf{z}\)</span>
following a distribution denoted by <span
class="math inline">\(\mathbb{P}\)</span>. For supervised learning, this
specializes to <span
class="math inline">\(\ell(\mathbf{w};\mathbf{z})=\ell(h(\mathbf{w};\mathbf{x}),y)\)</span>,
where <span
class="math inline">\(\mathbf{z}=(\mathbf{x},y)\)</span>.</p>
<div
style=" background-color: #f5f7fb; padding: 0.8em 1em; border-radius: 6px; margin: 1em 0;">
<p><strong>Definition 2.2 (Regularized DRO).</strong><br />
<span class="math display">\[\begin{align}\label{eqn:rdro}
\min_{\mathbf{w}}\hat{\mathcal{R}}_{\mathcal{S}}(\mathbf{w}):=\max_{\mathbf{p}\in\Delta_n}\sum_{i=1}^n
p_i\ell(\mathbf{w};\mathbf{z}_i)-\tau D_{\phi}\left(\mathbf{p}\|
\frac{\mathbf{1}}{n}\right).
\end{align}\]</span></p>
</div>
<div
style=" background-color: #f5f7fb; padding: 0.8em 1em; border-radius: 6px; margin: 1em 0;">
<p><strong>Definition 2.3 (Constrained DRO).</strong><br />
<span class="math display">\[\begin{align}\label{eqn:cdro}
\min_{\mathbf{w}}\hat{\mathcal{R}}_{\mathcal{S}}(\mathbf{w}):=&amp;\max_{\mathbf{p}\in\Omega}\sum_{i=1}^n
p_i\ell(\mathbf{w};\mathbf{z}_i)\\
\text{where }&amp;\quad\Omega=\left\{\mathbf{p}\mid
\mathbf{p}\in\Delta_n,D_{\phi}\left(\mathbf{p}\|
\frac{\mathbf{1}}{n}\right)\le\rho\right\}.\notag
\end{align}\]</span></p>
</div>
<p>The regularized DRO uses a regularization on the <span
class="math inline">\(\mathbf{p}\)</span> to implicitly define the
uncertainty set, and the constrained DRO uses a constraint on <span
class="math inline">\(\mathbf{p}\)</span> to explicitly define the
uncertainty set.</p>
<p>The maximization over <span class="math inline">\(\mathbf{p}\)</span>
in the DRO formulations simulates a worst-case scenario, thereby
enhancing the model‚Äôs robustness. The DRO objective interpolates between
the maximal loss and the average loss:</p>
<ul>
<li><p>Without the <span class="math inline">\(\phi\)</span>-divergence
regularization or constraint (i.e., <span
class="math inline">\(\tau=0\)</span> or <span
class="math inline">\(\rho=\infty\)</span>), the objective simplifies to
the maximal loss among all samples, which is particularly beneficial for
handling imbalanced data but is sensitive to outliers.</p></li>
<li><p>Conversely, when <span class="math inline">\(\rho=0\)</span> or
<span class="math inline">\(\tau=\infty\)</span>, the DRO objective
reduces to the standard empirical risk, which is not sensitive to
outliers but not suitable for imbalanced data.</p></li>
</ul>
<p>In practice, adding a tunable <span
class="math inline">\(\phi\)</span>-divergence regularization or
constraint (via tuning <span class="math inline">\(\tau\)</span> or
<span class="math inline">\(\rho\)</span>) increases the model‚Äôs
robustness.</p>
<p>A list of <span class="math inline">\(\phi\)</span>-divergence is
presented in <a href="#fig:tab:divergence">Table 2.1</a>. Two commonly
used ones in machine learning are presented below:</p>
<figure id="fig:tab:divergence">
<img src="assets/divergence-table.png" alt="Examples of phi-divergence."  style="width: 100%; max-width: 900px;">
</figure>
<ul>
<li><p>KL-Divergence: With <span class="math inline">\(\phi(t)=t\log
t-t+1\)</span>, the <span class="math inline">\(\phi\)</span>-divergence
becomes the KL divergence: <span class="math display">\[
\text{KL}(\mathbf{p},\mathbf{q})=D_{\phi}(\mathbf{p}\|\mathbf{q})=\sum_{i=1}^n
p_i\log(p_i/q_i).
\]</span></p></li>
<li><p>Conditional Value-at-Risk (CVaR): With <span
class="math inline">\(\phi(t)=\mathbb{I}_{0-\infty}(t\le1/\alpha)\)</span>,
where <span class="math inline">\(\alpha\in(0,1]\)</span> and <span
class="math inline">\(\mathbb{I}_{0-\infty}\)</span> is <span
class="math inline">\(0-\infty\)</span> indicator function, the
divergence becomes <span
class="math inline">\(D_{\phi}(\mathbf{p}\|\mathbf{q})=0\)</span> if
<span class="math inline">\(p_i\le q_i/\alpha,\forall i\)</span>,
otherwise <span
class="math inline">\(D_{\phi}(\mathbf{p}\|\mathbf{q})=\infty\)</span>.
The resulting DRO formulation is also known as the empirical CVaR-<span
class="math inline">\(\alpha\)</span>.</p></li>
</ul>
<h3 id="the-dual-form-of-regularized-dro">The Dual form of Regularized
DRO</h3>
<p>Solving the above DRO formulations requires dealing with a
high-dimensional variable <span
class="math inline">\(\mathbf{p}\)</span> from a simplex, which will
incur additional overhead compared with solving ERM when the number of
training data is large. The reason is that it requires performing a
projection onto the simplex <span
class="math inline">\(\Delta_n\)</span> or the constrained simplex <span
class="math inline">\(\Omega=\{\mathbf{p}\in\Delta_n,D_{\phi}\left(\mathbf{p}\|
\frac{\mathbf{1}}{n}\right)\le\rho\}\)</span>. To reduce this overhead,
one approach is to convert the problem into unconstrained one using the
Langrangian dual theory based on the convex conjugate of <span
class="math inline">\(\phi\)</span> function.</p>
<div id="prop:1"
style="border: 5px solid #ccc; padding: 0.2em; overflow-x: auto; border-radius: 6px; background-color: #eef4fc;">
<p><strong><strong>Proposition 2.1</strong> </strong> (Dual form of
Regularized DRO)<br />
Let <span
class="math inline">\(\phi^*(s)=\max_{t\ge0}ts-\phi(t)\)</span>. Then we
have <span class="math display">\[\begin{align}\label{eqn:dual-rdro}
\max_{\mathbf{p}\in\Delta_n}\sum_{i=1}^n
p_i\ell(\mathbf{w};\mathbf{z}_i)-\tau D_{\phi}\left(\mathbf{p}\|
\frac{\mathbf{1}}{n}\right)=\min_{\nu}\frac{\tau}{n}\sum_{i=1}^n\phi^*\bigg(\frac{\ell(\mathbf{w};\mathbf{z}_i)-\nu}{\tau}\bigg)+\nu.
\end{align}\]</span></p>
</div>
<p>The proof can be found in <a
href="Ch1-4.html#example1-14-dro">Example 1.14</a>.</p>
<h3 id="examples-of-regularized-dro">Examples of Regularized DRO</h3>
<h4 id="exp-klrdro"><strong>Example 2.1: KL-divergence Regularized
DRO</strong></h4>
<p>For the special case of using KL-divergence, we can further simplify
the above objective function. Since <span
class="math inline">\(\phi(t)=t\log t-t+1\)</span>, then <span
class="math inline">\(\phi^*(s)=\exp(s)-1\)</span> (See <a
href="Ch1-4.html#example1-15-conj-phi">Example 1.15</a>) and solving
<span class="math inline">\(\nu\)</span> yields <span
class="math display">\[
\nu=\tau\log\bigg(\frac{1}{n}\sum_{i=1}^n\exp(\ell(\mathbf{w};\mathbf{z}_i)/\tau)\bigg).
\]</span> Plugging it back into the objective, we can obtain a
simplified form <span class="math display">\[
\max_{\mathbf{p}\in\Delta_n}\sum_{i=1}^n
p_i\ell(\mathbf{w};\mathbf{z}_i)-\tau\text{KL}\left(\mathbf{p},\frac{\mathbf{1}}{n}\right)
=\tau\log\bigg(\frac{1}{n}\sum_{i=1}^n\exp(\ell(\mathbf{w};\mathbf{z}_i)/\tau)\bigg).
\]</span> As a result, with <span class="math inline">\(\phi(t)=t\log
t-t+1\)</span>, the KL-divergence regularized DRO (<span
class="math inline">\(\ref{eqn:rdro}\)</span>) is equivalent to <span
class="math display">\[\begin{align}\label{eqn:klrdro}
\min_{\mathbf{w}}\tau\log\left(\frac{1}{n}\sum_{i=1}^n\exp\left(\frac{\ell(\mathbf{w};\mathbf{z}_i)}{\tau}\right)\right).
\end{align}\]</span></p>
<h4 id="exp-cvar"><strong>Example 2.2: Empirical CVaR</strong></h4>
<p>As another example, we derive the dual form of the empirical CVaR.
With simple algebra, we can derive that <span
class="math inline">\(\phi^*(s)=\frac{[s]_+}{\alpha}\)</span> (See <a
href="Ch1-4.html#example1-15-conj-phi">Example 1.15</a>) for <span
class="math inline">\(\phi(t)=\mathbb{I}_{0-\infty}(t\le1/\alpha)\)</span>.</p>
<p>As a result, with <span
class="math inline">\(\phi(t)=\mathbb{I}_{0-\infty}(t\le1/\alpha)\)</span>,
the regularized DRO (<span
class="math inline">\(\ref{eqn:rdro}\)</span>) corresponding to the
empirical CVaR-<span class="math inline">\(\alpha\)</span> is equivalent
to <span class="math display">\[\begin{align}\label{eqn:cvar}
\min_{\mathbf{w},\nu}\frac{1}{n\alpha}\sum_{i=1}^n[\ell(\mathbf{w};\mathbf{z}_i)-\nu]_+
+ \nu.
\end{align}\]</span> When <span
class="math inline">\(k=n\alpha\in[1,n]\)</span> is an integer, the
above objective reduces to the average of top-<span
class="math inline">\(k\)</span> loss values when sorting them in
descending order, as shown in the following lemma.</p>
<div id="lem:cvar-top-k"
style="border: 5px solid #ccc; padding: 0.2em; overflow-x: auto; border-radius: 6px; background-color: #eef4fc;">
<p><strong><strong>Lemma 2.2</strong> </strong><br />
Let <span class="math inline">\(\ell_{[i]}\)</span> denote the <span
class="math inline">\(i\)</span>-th largest loss among <span
class="math inline">\(\{\ell(\mathbf{w};\mathbf{z}_i),i=1,\ldots,n\}\)</span>
ranked in descending order. If <span
class="math inline">\(\alpha=k/n\)</span>, we have <span
class="math display">\[\begin{align}\label{eqn:cvar-top-k}
\min_{\nu}\frac{1}{n\alpha}\sum_{i=1}^n[\ell(\mathbf{w};\mathbf{z}_i)-\nu]_+
+ \nu=\frac{1}{k}\sum_{i=1}^k\ell_{[i]}.
\end{align}\]</span></p>
</div>
<p><strong>Proof.</strong><br />
First, we have <span class="math display">\[
\min_{\nu}\frac{1}{n\alpha}\sum_{i=1}^n[\ell(\mathbf{w};\mathbf{z}_i)-\nu]_+
+ \nu
=\min_{\nu}\frac{1}{n\alpha}\sum_{i=1}^n[\ell_{[i]}-\nu]_+ + \nu.
\]</span> Let <span class="math inline">\(\nu_*\)</span> be an optimal
solution given <span class="math inline">\(\mathbf{w}\)</span>. Due to
the first-order optimality condition, we have <span
class="math display">\[\begin{align*}
0\in\frac{1}{k}\sum_{i=1}^n\partial_{\nu}[\ell_{[i]}-\nu_*]_+ + 1.
\end{align*}\]</span> Hence, <span
class="math display">\[\begin{align}\label{eqn:opt-mu}
-k\in\sum_{i=1}^n\partial_{\nu}[\ell_{[i]}-\nu_*]_+.
\end{align}\]</span> Let us first assume <span
class="math inline">\(\ell_{[k+1]}&lt;\ell_{[k]}\)</span>. We will show
that <span
class="math inline">\(\nu_*\in(\ell_{[k+1]},\ell_{[k]}]\)</span> satisfy
this condition. Since <span
class="math inline">\(-1\in\partial_{\nu}[\ell_{[i]}-\nu_*]_+\)</span>
for <span class="math inline">\(i=1,\ldots,k\)</span> due to <span
class="math inline">\(\ell_{[i]}\ge\nu_*\)</span> and <span
class="math inline">\(\partial_{\nu}[\ell_{[i]}-\nu_*]_+=0\)</span> for
<span class="math inline">\(i=k+1,\ldots,n\)</span> due to <span
class="math inline">\(\ell_{[i]}&lt;\nu_*\)</span>. Hence, it verifies
that the condition(<span
class="math inline">\(\ref{eqn:opt-mu}\)</span>) holds at such <span
class="math inline">\(\nu_*\)</span>.</p>
If <span class="math inline">\(\ell_{[k+1]}=\ell_{[k]}\)</span>, we
argue that <span class="math inline">\(\nu_*=\ell_{[k]}\)</span> can
still satisfy(<span class="math inline">\(\ref{eqn:opt-mu}\)</span>).
This is because <span
class="math inline">\(-1\in\partial_{\nu}[\ell_{[i]}-\nu_*]_+\)</span>
for <span class="math inline">\(i=1,\ldots,k\)</span> and <span
class="math inline">\(0\in\partial_{\nu}[\ell_{[i]}-\nu_*]_+\)</span>
for <span class="math inline">\(\ell_{[i]}=\ell_{[k+1]},i\ge
k+1\)</span> and <span
class="math inline">\(\partial_{\nu}[\ell_{[i]}-\nu_*]_+=0\)</span> for
<span class="math inline">\(\ell_{[i]}&lt;\ell_{[k+1]},i\ge
k+1\)</span>. Then the conclusion follows.<br />

<p style="text-align: right;">
‚ñ†
</p>
<h3 id="the-dual-form-of-constrained-dro">The Dual form of Constrained
DRO</h3>
<p>For transforming the constrained DRO, we can use the following
proposition based on the Lagrangian duality theory.</p>
<div
style="border: 5px solid #ccc; padding: 0.2em; overflow-x: auto; border-radius: 6px; background-color: #eef4fc;">
<p><strong><strong>Proposition 2.2</strong> </strong> (Dual form of
Constrained DRO)<br />
Let <span
class="math inline">\(\phi^*(s)=\max_{t\ge0}ts-\phi(t)\)</span>. Then we
have <span class="math display">\[\begin{align}\label{eqn:dual-cdro}
\max_{\mathbf{p}\in\Delta_n,D_{\phi}\left(\mathbf{p}\|
\frac{\mathbf{1}}{n}\right)\le\rho}\sum_{i=1}^n
p_i\ell(\mathbf{w};\mathbf{z}_i)
=\min_{\tau\ge0,\nu}\frac{\tau}{n}\sum_{i=1}^n\phi^*\bigg(\frac{\ell(\mathbf{w};\mathbf{z}_i)-\nu}{\tau}\bigg)+\nu+\tau\rho.
\end{align}\]</span></p>
</div>
<p>The proof is similar to that of See <a href="#prop:1">Proposition
2.1</a>.</p>
<h3 id="examples-of-constrained-dro">Examples of Constrained DRO</h3>
<h4 id="exp-klcdro"><strong>Example 2.3: KL Constrained
DRO</strong></h4>
<p>With <span class="math inline">\(\phi(t)=t\log t-t+1\)</span>, the
KL-divergence constrained DRO (<span
class="math inline">\(\ref{eqn:cdro}\)</span>) is equivalent to: <span
class="math display">\[\begin{align}\label{eqn:klcdro}
\min_{\mathbf{w},\tau\ge0}\tau\log\left(\frac{1}{n}\sum_{i=1}^n\exp\left(\frac{\ell(\mathbf{w};\mathbf{z}_i)}{\tau}\right)\right)+\tau\rho.
\end{align}\]</span></p>
<p>KL-regularized DRO and KL-constrained DRO play important roles in
many modern artificial intelligence applications. The <a
href="Ch2-1.html#eqn-ldr">LDR loss</a> can be interpreted as a form of
KL-regularized DRO, except that the uncertainty is placed on the
distribution of class labels for each individual data point. We will
present additional applications in <a href="Ch2-4.html">Section
2.4</a>.</p>
<h3 id="the-optimization-challenge">The Optimization Challenge</h3>
<p>Although the transformed optimization problems do not involve dealing
with a high-dimensional variable <span
class="math inline">\(\mathbf{p}\in\Delta_n\)</span>, the new
optimization problems (<span
class="math inline">\(\ref{eqn:klrdro}\)</span>), (<span
class="math inline">\(\ref{eqn:klcdro}\)</span>) are not of the same
form as ERM. The critical assumption that an unbiased gradient can be
easily computed fails. We will cast them as instances of stochastic
compositional optimization (SCO), which is topic of <a
href="chapter4.html">Chapter 4</a> of the book.</p>
<h2 id="optimized-certainty-equivalent">Optimized Certainty
Equivalent</h2>
<p>How to understand the generalization of DRO? One way is to still
consider bounding the expected risk <span
class="math inline">\(\mathcal{R}(\mathbf{w})\)</span> of the learned
model. However, the expected risk may not be a good measure when the
data distribution is skewed.</p>
<p>For simplicity, let us consider a binary classification problem with
<span
class="math inline">\(\Pr(\mathbf{x},y=1)=\pi_+\Pr(\mathbf{x}|y=1)\)</span>
and <span
class="math inline">\(\Pr(\mathbf{x},y=-1)=\pi_-\Pr(\mathbf{x}|y=-1)\)</span>,
where <span
class="math inline">\(\pi_+=\Pr(y=1),\pi_-=\Pr(y=-1)\)</span>. Let <span
class="math inline">\(\mathbb{P}_+\)</span> and <span
class="math inline">\(\mathbb{P}_-\)</span> be the distributions of
<span class="math inline">\(\mathbf{x}\)</span> conditioned on <span
class="math inline">\(y=1\)</span> and <span
class="math inline">\(y=-1\)</span>, respectively. By the law of total
expectation we have <span class="math display">\[\begin{align}
\mathcal{R}(\mathbf{w})=\mathbb{E}_{\mathbf{x},y}\ell(h(\mathbf{w};\mathbf{x}),y)
=\pi_+\mathbb{E}_{\mathbf{x}\sim\mathbb{P}_+}[\ell(h(\mathbf{w};\mathbf{x}),1)]
+\pi_-\mathbb{E}_{\mathbf{x}\sim\mathbb{P}_-}[\ell(h(\mathbf{w};\mathbf{x}),-1)].
\end{align}\]</span> If <span
class="math inline">\(\pi_-\gg\pi_+\)</span>, the expected risk would be
dominated by the expected loss of data from the negative class. As a
result, a small <span
class="math inline">\(\mathcal{R}(\mathbf{w})\)</span> does not
necessarily indicate a small <span
class="math inline">\(\mathbb{E}_{\mathbf{x}\sim\mathbb{P}_+}[\ell(\mathbf{w};\mathbf{x},1)]\)</span>.</p>
<p>Instead, we consider the population risk of DRO as the target
measure. A formal definition of the population risk for the regularized
DRO(<span class="math inline">\(\ref{eqn:rdro}\)</span>) is given
below.</p>
<div
style=" background-color: #f5f7fb; padding: 0.8em 1em; border-radius: 6px; margin: 1em 0;">
<p><strong>Definition 2.4 (Population risk of DRO).</strong><br />
Given a data distribution <span
class="math inline">\(\mathbb{P}\)</span>, for any <span
class="math inline">\(\tau&gt;0\)</span>, we define the population risk
of regularized DRO (<span class="math inline">\(\ref{eqn:rdro}\)</span>)
as: <span class="math display">\[\begin{align}
\mathcal{R}_{\text{oce}}(\mathbf{w}):=&amp;\max_{\mathbb{Q}\in\mathcal{Q}}\mathbb{E}_{\mathbf{z}&#39;\sim\mathbb{Q}}\ell(\mathbf{w};\mathbf{z}&#39;)-\tau\mathbb{E}_{\mathbb{P}}\phi\left(\frac{d\mathbb{Q}}{d\mathbb{P}}\right)\\
=&amp;\min_{\nu}\tau\mathbb{E}_{\mathbf{z}\sim\mathbb{P}}\phi^*\left(\frac{\ell(\mathbf{w};\mathbf{z})-\nu}{\tau}\right)+\nu,\label{eqn:oce}
\end{align}\]</span> where <span
class="math inline">\(\phi^*(s)=\max_{t\ge0}ts-\phi(t)\)</span>.</p>
</div>
<p>In the definition above, <span
class="math inline">\(\mathcal{Q}=\{\mathbb{Q}\mid \mathbb{Q}\ll
\mathbb{P}\}\)</span> denotes the set of probability measures that are
absolutely continuous with respect to <span
class="math inline">\(\mathbb{P}\)</span>. A probability measure <span
class="math inline">\(\mathbb{Q}\)</span> is said to be absolutely
continuous with respect to <span
class="math inline">\(\mathbb{P}\)</span>, denoted <span
class="math inline">\(\mathbb{Q}\ll \mathbb{P}\)</span>, if every event
that has probability <span class="math inline">\(0\)</span> under <span
class="math inline">\(\mathbb{P}\)</span> also has probability <span
class="math inline">\(0\)</span> under <span
class="math inline">\(\mathbb{Q}\)</span>. If <span
class="math inline">\(\mathbb{P}\)</span> and <span
class="math inline">\(\mathbb{Q}\)</span> admit densities <span
class="math inline">\(p(z)\)</span> and <span
class="math inline">\(q(z)\)</span> with respect to a common dominating
measure on <span class="math inline">\(\mathcal{Z}\)</span>, and <span
class="math inline">\(\mathbb{Q}\ll \mathbb{P}\)</span>, then <span
class="math display">\[
\mathbb{E}_{\mathbb{P}}\left[\phi\left(\frac{d\mathbb{Q}}{d\mathbb{P}}\right)\right]
=\int_{\mathcal{Z}}p(z)\phi\left(\frac{q(z)}{p(z)}\right)\,dz.
\]</span></p>
<p>The equivalent counterpart in (<span
class="math inline">\(\ref{eqn:oce}\)</span>) is a risk measure
originates from the optimized certainty equivalent (OCE), a concept
popularized in mathematical economics <a
href="Ch2-5.html#ref26">(Ben-Tal and Teboulle 2007)</a>. Minimizing OCE
has an effect of so-called risk-aversion, which discourages models from
having rare but catastrophic errors. Two special cases are discussed
below:</p>
<ul>
<li><p>When <span
class="math inline">\(\phi(t)=\mathbb{I}_{0-\infty}(t\le1/\alpha)\)</span>,
the OCE becomes the CVaR-<span class="math inline">\(\alpha\)</span>,
i.e., <span class="math display">\[
\mathcal{R}_{\text{cvar}}(\mathbf{w})=\mathbb{E}_{\mathbf{z}}[\ell(\mathbf{w};\mathbf{z})|\ell(\mathbf{w};\mathbf{z})\ge\text{VAR}_{\alpha}(\ell(\mathbf{w};\mathbf{z}))],
\]</span> where <span
class="math inline">\(\text{VAR}_{\alpha}(\ell(\mathbf{w};\mathbf{z}))=\sup_s[\Pr(\ell(\mathbf{w};\mathbf{z})\ge
s)\ge\alpha]\)</span> is the <span
class="math inline">\(\alpha\)</span>-quantile or ‚Äúvalue-at-risk‚Äù of the
random loss values.</p></li>
<li><p>When <span class="math inline">\(\phi(t)=t\log t-t+1\)</span>,
OCE becomes the entropic risk: <span class="math display">\[
\mathcal{R}_{\text{ent}}(\mathbf{w})=\tau\log\left(\mathbb{E}_{\mathbf{z}}\exp\left(\frac{\ell(\mathbf{w};\mathbf{z})}{\tau}\right)\right).
\]</span></p></li>
</ul>
<div
style="background-color:#f2f7ff; border:1px solid #1f4aa8; border-radius:0px; padding:0.6em 0.8em; overflow-x:auto;">
<div style="font-weight:700; margin-bottom:0.4em;">
What is risk-aversion?
</div>
Risk aversion refers to the preference for a certain and predictable
cost over an uncertain outcome with the same average cost, especially
when the uncertainty involves rare but severe losses. This behavior
cannot be captured by the expectation alone, which treats all outcomes
linearly and ignores tail risk. The OCE provides a principled
risk-sensitive alternative by assigning a single certainty-equivalent
value to a random loss that accounts for both its mean and its
variability. A classic illustration is insurance: consider paying a
fixed premium of <span class="math inline">\(1,000\)</span> versus
facing a <span class="math inline">\(100,000\)</span> medical bill with
probability <span class="math inline">\(0.01\)</span> and zero cost
otherwise. Although both options have the same expected cost, the OCE
risk <span class="math inline">\(\log \mathbb{E}[\exp(X)]\approx
99995\)</span> assigns a much larger value to the uninsured option, as
it heavily penalizes the rare catastrophic loss. Consequently, OCE
correctly reflects the economic rationale behind insurance decisions by
favoring stable outcomes over risky alternatives with heavy tails.
</div>
<p>We present two properties of OCE below.</p>
<div id="lem:oce-1"
style="border: 5px solid #ccc; padding: 0.2em; overflow-x: auto; border-radius: 6px; background-color: #eef4fc;">
<p><strong><strong>Lemma 2.3</strong> </strong><br />
Let <span
class="math inline">\(\partial\phi^*(t)=\{s:\phi&#39;^{*}_-(t)\le
s\le\phi&#39;^{*}_+(t)\}\)</span>. If <span
class="math inline">\(a&lt;b\)</span>, then <span
class="math inline">\(0\le\phi&#39;^*_+(a)\le\phi&#39;^*_-(b)\)</span>.</p>
</div>
<strong>Proof.</strong><br />
Due to the definition <span
class="math inline">\(\phi^*(s)=\max_{t\ge0}ts-\phi(t)\)</span>, we have
<span class="math inline">\(\partial\phi^*(s)\ge0\)</span>, which
indicates that <span class="math inline">\(\phi^*\)</span> is
non-decreasing. Since <span class="math inline">\(\phi^*\)</span> is
also convex, the conclusion follows from the convex analysis <a
href="Ch2-5.html#ref58">(Rockafellar, 1970)</a>.<br />

<p style="text-align: right;">
‚ñ†
</p>
<div id="lem:oce"
style="border: 5px solid #ccc; padding: 0.2em; overflow-x: auto; border-radius: 6px; background-color: #eef4fc;">
<p><strong><strong>Lemma 2.4</strong> </strong><br />
For any <span
class="math inline">\(\tau&gt;0,\mathbf{w}\in\mathbb{R}^d\)</span>, it
holds that <span
class="math inline">\(\mathcal{R}_{\text{oce}}(\mathbf{w})\ge\mathcal{R}(\mathbf{w})\)</span>.</p>
</div>
<strong>Proof.</strong><br />
Since <span class="math inline">\(\phi(1)=0\)</span>, then <span
class="math inline">\(\phi^*(s)=\max_{t\ge0}ts-\phi(t)\ge
s-\phi(1)=s\)</span>. Hence, <span class="math display">\[\begin{align*}
\mathcal{R}_{\text{oce}}(\mathbf{w})
&amp;=\min_{\nu}\tau\mathbb{E}_{\mathbf{z}}\phi^*\left(\frac{\ell(\mathbf{w};\mathbf{z})-\nu}{\tau}\right)+\nu\\
&amp;\ge\min_{\nu}\tau\mathbb{E}_{\mathbf{z}}\left(\frac{\ell(\mathbf{w};\mathbf{z})-\nu}{\tau}\right)+\nu
=\mathcal{R}(\mathbf{w}).
\end{align*}\]</span>
<p style="text-align: right;">
‚ñ†
</p>
<div
style="border: 2px solid #aaa; padding: 0.6em 0.8em; overflow-x: auto; border-radius: 6px; background-color: #f0f0f0;">
<p><strong>üí° Why it matters</strong><br />
<a href="#lem:oce-1">Lemma 2.3</a> implies that a data with a larger
loss <span
class="math inline">\(\ell(h(\mathbf{w};\mathbf{x}),y)\)</span> will
have a higher weight in the gradient calculation in terms of <span
class="math inline">\(\mathbf{w}\)</span>.</p>
<p><a href="#lem:oce">Lemma 2.4</a> indicates that OCE is a stronger
measure than the expected risk. A small OCE will imply a small expected
risk, while the reverse is not necessarily true.</p>
</div>
<p>Based on OCE, we can define the excess risk <span
class="math inline">\(\mathcal{R}_{\text{oce}}(\mathbf{w})-\min_{\mathbf{u}\in\mathcal{W}}\mathcal{R}_{\text{oce}}(\mathbf{u})\)</span>
and decompose it into an optimization error and a generalization error
similar to <a href="Ch2-1.html#lem:decomp">Lemma 2.1</a>.</p>
<div
style="border: 5px solid #ccc; padding: 0.2em; overflow-x: auto; border-radius: 6px; background-color: #eef4fc;">
<p><strong><strong>Lemma 2.5</strong> </strong><br />
For a learned model <span
class="math inline">\(\mathbf{w}=\mathcal{A}(\mathcal{S};\zeta)\)</span>
for solving empirical DRO (<span
class="math inline">\(\ref{eqn:rdro}\)</span>), we have <span
class="math display">\[\begin{align*}
\mathcal{R}_{\text{oce}}(\mathbf{w})-\min_{\mathbf{u}\in\mathcal{W}}\mathcal{R}_{\text{oce}}(\mathbf{u})
\le2\underbrace{\sup_{\mathbf{w}\in\mathcal{W}}|\mathcal{R}_{\text{oce}}(\mathbf{w})-\hat{\mathcal{R}}_{\mathcal{S}}(\mathbf{w})|}\limits_{\text{generalization
error}}
+\underbrace{\hat{\mathcal{R}}_{\mathcal{S}}(\mathbf{w})-\min_{\mathbf{u}\in\mathcal{W}}\hat{\mathcal{R}}_{\mathcal{S}}(\mathbf{u})}\limits_{\text{optimization
error}}.
\end{align*}\]</span></p>
</div>
<h2 id="ch2-sec-gdro">Group Distributionally Robust Optimization</h2>
<p>Group DRO is an extension of DRO by aggregating data into groups and
using DRO on the group level to formulate a robust risk function. It is
helpful to promote equity of the learned model and mitigating the impact
of spurious correlations that exist between the label and some features,
by using prior knowledge to group the data.</p>
<p>Let us consider an illustrative example of classifying waterbird
images from landbird images (see <a href="#fig:spucor">Figure 2.2</a>).
The training data may have the same number of waterbird images and
landbird images. However, most waterbird images may have water in the
background and most landbird images may have land in the background.
Standard empirical risk minimization may learn spurious correlation
between the class labels (e.g., waterbird) and the specific value of
some attribute (e.g., the water background). As a consequence, the model
may perform poorly on waterbird images with land background.</p>
<div
style="background-color:#f2f7ff; border:1px solid #1f4aa8; border-radius:0px; padding:0.6em 0.8em; overflow-x:auto;">
<div style="font-weight:700; margin-bottom:0.4em;">
Critical:
</div>
Data may exhibit imbalance not in the marginal distribution of class
label but some joint distribution of the class label and some
attributes, which causes the spurious correlation.
</div>
<figure id="fig:spucor">
<img src="assets/gdro.png" alt="Illustrative of spurious correlation between the class label and some feature: waterbird images mostly have water background and landbird images mostly have land background."  style="width: 100%; max-width: 900px;">
<figcaption style="text-align: center; font-style: italic; margin-top: 0.5em;">
Fig 2.2: Illustrative of spurious correlation between the class label
and some feature: waterbird images mostly have water background and
landbird images mostly have land background.
</figcaption>
</figure>
<p>GDRO can be used to mitigate this issue by leveraging prior knowledge
of spurious correlations to define groups over the training data. Let
the training data be divided into multiple groups <span
class="math inline">\(\mathcal{G}_1,\ldots,\mathcal{G}_K\)</span>, where
<span
class="math inline">\(\mathcal{G}_j=\{(\mathbf{x}^j_1,y^j_1),\ldots,(\mathbf{x}^j_{n_j},y^j_{n_j})\}\)</span>
includes a set of examples from the <span
class="math inline">\(j\)</span>-th group. We define an averaged loss
over examples from each group <span
class="math inline">\(L_j(\mathbf{w})=\frac{1}{n_j}\sum_{i=1}^{n_j}\ell(h(\mathbf{w};\mathbf{x}^j_i),y^j_i)\)</span>.
Then, a regularized group DRO can be defined as <span
class="math display">\[\begin{align}\label{eqn:rgdro}
\min_{\mathbf{w}}\max_{\mathbf{p}\in\Delta_K}\sum_{j=1}^K p_j
L_j(\mathbf{w})-\tau D_{\phi}\left(\mathbf{p}\|
\frac{\mathbf{1}}{K}\right),
\end{align}\]</span> and a constrained group DRO is given by: <span
class="math display">\[\begin{equation}\label{eqn:cgdro}
\begin{aligned}
\min_{\mathbf{w}}\max_{\mathbf{p}\in\Delta_K,D_{\phi}(\mathbf{p}\|
\frac{\mathbf{1}}{K})\le\rho}\sum_{j=1}^K p_j L_j(\mathbf{w}).
\end{aligned}
\end{equation}\]</span> By doing so, the learning process is less likely
to be dominated by the majority group associated with the spurious
correlation between the label and a particular feature (e.g., waterbird
images with water background). If the model only captures the spurious
correlation, the loss for the minority group will be large, which in
turn drives the learning process to reduce this loss and thereby
mitigate the spurious correlation.</p>
<h3 id="examples-and-reformulations">Examples and Reformulations</h3>
<p>Similar to before, we can convert the min-max problem into a
minimization problem to reduce additional overhead of dealing with a
large number of groups. We give two examples of using the KL-divergence
constraint of <span class="math inline">\(\mathbf{p}\)</span> and
CVaR-<span class="math inline">\(\alpha\)</span>.</p>
<p>With <span class="math inline">\(\phi(t)=t\log t-t+1\)</span>, the
KL-divergence constrained group DRO (<span
class="math inline">\(\ref{eqn:cgdro}\)</span>) is equivalent to <span
class="math display">\[\begin{align}\label{eqn:klgdro}
\min_{\mathbf{w},\tau\ge0}\tau\log\left(\frac{1}{K}\sum_{j=1}^K\exp\left(\frac{L_j(\mathbf{w})}{\tau}\right)\right)+\tau\rho.
\end{align}\]</span> With <span
class="math inline">\(\phi(t)=\mathbb{I}_{0-\infty}(t\le1/\alpha)\)</span>,
CVaR-<span class="math inline">\(\alpha\)</span> group DRO (<span
class="math inline">\(\ref{eqn:cgdro}\)</span>) is equivalent to <span
class="math display">\[\begin{align}\label{eqn:cvargdro}
\min_{\mathbf{w},\nu}\frac{1}{K\alpha}\sum_{j=1}^K[L_j(\mathbf{w})-\nu]_+
+ \nu.
\end{align}\]</span></p>
<h3 id="the-optimization-challenge-1">The Optimization Challenge</h3>
<p>Again, these new optimization problems (<span
class="math inline">\(\ref{eqn:klgdro}\)</span>), (<span
class="math inline">\(\ref{eqn:cvargdro}\)</span>) cannot be solved by
simply using existing stochastic algorithms for ERM since <span
class="math inline">\(L_j(\mathbf{w})\)</span> depends on many data and
they are inside non-linear functions. In particular, the problem (<span
class="math inline">\(\ref{eqn:cvargdro}\)</span>) is an instance of
finite-sum coupled compositional optimization (FCCO), which will be
explored in <a href="chapter5.html">Chapter 5</a> in depth.</p>
<p style="text-align:left; margin-top:1.5em;">
<a href="javascript:history.back()">‚Üê Go Back</a>
</p>
</article>
</body>
</html>
